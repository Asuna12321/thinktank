{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "paper_df = pd.read_csv(\"paper.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "paper_df['city'] = ['sh' if i < 10000 else 'bj' for i in range(len(paper_df))]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "grouped_paper = paper_df.groupby([\"year\", \"city\"])\n",
    "\n",
    "df1 = pd.DataFrame()\n",
    "count = grouped_paper[\"index id\"].apply(len)\n",
    "df1[\"year\"] = [idx[0] for idx in count.axes[0]]\n",
    "df1[\"city\"] = [idx[1] for idx in count.axes[0]]\n",
    "df1[\"publication count\"] = count.values"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "JS_COL = ['id', 'authors', 'venue', 'year', 'keywords', 'references', 'n_citation', 'doc_type']\n",
    "PD_COL = ['id', 'author_id', 'author_org', 'venue', 'year', 'keywords', 'references', 'n_cites', 'doc_type']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def parse(js):\n",
    "    processed_line = dict()\n",
    "    for prefix in JS_COL:\n",
    "        # Only keywords in the list above are considered\n",
    "        # If more keywords are required, according read_in code should be added\n",
    "        if prefix == 'id':\n",
    "            processed_line['id'] = js.get('id', np.nan)\n",
    "        elif prefix == 'authors':\n",
    "            authors = js.get('authors', list())\n",
    "            processed_line['authors_id'] = ', '.join([str(author.get('id', np.nan))\n",
    "                                                      for author in authors])\n",
    "            processed_line['authors_org'] = ', '.join([str(author.get('org', np.nan))\n",
    "                                                      for author in authors])\n",
    "        elif prefix == 'venue':\n",
    "            venue = js.get('venue', dict())\n",
    "            processed_line['venue_id'] = venue.get('id', np.nan)\n",
    "            processed_line['venue_name'] = venue.get('raw', np.nan)\n",
    "        elif prefix == 'year':\n",
    "            processed_line['year'] = js.get('year', np.nan)\n",
    "        elif prefix == 'keywords':\n",
    "            processed_line['keywords'] = ', '. join(js.get('keywords', list()))\n",
    "        elif prefix == 'references':\n",
    "            processed_line['references'] = ', '. join([str(r) for r in js.get('references', list())])\n",
    "        elif prefix == 'n_citation':\n",
    "            processed_line['n_cites'] = js.get('n_cites', np.nan)\n",
    "        elif prefix == 'doc_type':\n",
    "            processed_line['doc_type'] = js.get('doc_type', np.nan)\n",
    "    return processed_line"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'parse' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-2-bc639249d707>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     12\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mline\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     13\u001B[0m             \u001B[0mjs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mjson\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mloads\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mline\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 14\u001B[1;33m             \u001B[0mm\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mparse\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mjs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     15\u001B[0m             \u001B[0mparsed_df\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mparsed_df\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mm\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mignore_index\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mTrue\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     16\u001B[0m         \u001B[0mi\u001B[0m \u001B[1;33m+=\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'parse' is not defined"
     ]
    }
   ],
   "source": [
    "parsed_df = pd.DataFrame()\n",
    "i = 0\n",
    "with open(r'D:\\Dataset\\ThinkTank\\dblp.v12.json', encoding='utf-8') as f:\n",
    "    while True and i < 10:\n",
    "        line = f.readline()\n",
    "        if not line:\n",
    "            break\n",
    "        line = line.lstrip('[')\n",
    "        line = line.lstrip(',')\n",
    "        line = line.rstrip(']')\n",
    "        line = line.rstrip('\\n')\n",
    "        if line:\n",
    "            js = json.loads(line)\n",
    "            m = parse(js)\n",
    "            parsed_df = parsed_df.append(m, ignore_index=True)\n",
    "        i += 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "2"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "js.get('ids', 2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "with open(r'D:\\Dataset\\ThinkTank\\AMiner-Coauthor.txt', encoding='utf-8') as f:\n",
    "    dat = f.readlines()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "def parse_coauthor(file):\n",
    "    \"\"\" Parse & convert coauthor into Dataframe.\n",
    "\n",
    "        Args:\n",
    "            -- file: coauthor file address, encoding in utf-8.\n",
    "                coauthor file: https://lfs.aminer.cn/lab-datasets/aminerdataset/AMiner-Coauthor.zip\n",
    "\n",
    "        \"\"\"\n",
    "    coauthor_df = pd.DataFrame()\n",
    "    with open(file, encoding='utf-8') as f:\n",
    "        data = f.readlines()\n",
    "    print(len(data))\n",
    "    coauthor_df = coauthor_df.append([c.lstrip('#').rstrip('\\n').split('\\t') for c in data])\n",
    "    coauthor_df.columns = ['1st', '2nd', 'num']\n",
    "    coauthor_df.astype('int64')\n",
    "    return coauthor_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4258946\n",
      "7.878896713256836\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "coauthor_df = parse_coauthor(r'D:\\Dataset\\ThinkTank\\AMiner-Coauthor.txt')\n",
    "print(time.time()-start)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "coauthor_df.to_csv('coauthor.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "processed_line = dict()\n",
    "fos = js['fos']\n",
    "processed_line['fos_name'] = ';'.join([str(f.get('name', None))\n",
    "                                       for f in fos if f.get('name', None)])\n",
    "processed_line['fos_weight'] = ';'.join([str(f.get('w', None))\n",
    "                                         for f in fos if f.get('w', None)])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "['Telecommunications network',\n 'Computer science',\n 'Mind map',\n 'Humanâ€“computer interaction',\n 'Multimedia',\n 'Empirical research',\n 'Comprehension',\n 'Communications protocol']"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[str(f.get('name', None)) for f in fos if f.get('name', None)]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "'Telecommunications network;Computer science;Mind map;Humanâ€“computer interaction;Multimedia;Empirical research;Comprehension;Communications protocol'"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "';'.join([str(f.get('name', None)) for f in fos if f.get('name', None)])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "a = pd.DataFrame()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "a['ll'] = 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def parsejson2df(file, col_lst=JSON_KEYWORDS):\n",
    "    \"\"\" Parse & load json file to Dataframe.\n",
    "\n",
    "    Data at https://originalstatic.aminer.cn/misc/dblp.v12.7z\n",
    "    ** THIS FILE IS PRETTY LARGE (12GB when unzipped), the file is loaded by readline,\n",
    "    HOWEVER THE JSON DATA IS CONVERTED TO A SINGLE DATAFRAME, MAY REQUIRE LARGE MEMORY.\n",
    "\n",
    "    ETA 30min\n",
    "    \"\"\"\n",
    "\n",
    "    def parse(js):\n",
    "        processed_line = dict()\n",
    "        for col in set(col_lst).intersection(set(js.keys())):\n",
    "            # Only keywords in the list above are considered\n",
    "            # If more keywords are required, according read_in code should be added\n",
    "            if col == 'id':\n",
    "                processed_line['id'] = js['id']\n",
    "            elif col == 'authors':\n",
    "                authors = js['authors']\n",
    "                processed_line['authors_id'] = ';'.join([str(author.get('id', None))\n",
    "                                                         for author in authors if author.get('id', None)])\n",
    "                processed_line['authors_name'] = ';'.join([author.get('name', None)\n",
    "                                                           for author in authors if author.get('name', None)])\n",
    "                processed_line['authors_org'] = ';'.join([str(author.get('org', None))\n",
    "                                                          for author in authors if author.get('org', None)])\n",
    "            elif col == 'venue':\n",
    "                venue = js['venue']\n",
    "                processed_line['venue_id'] = venue.get('id', None)\n",
    "                processed_line['venue_name'] = venue.get('raw', None)\n",
    "            elif col == 'year':\n",
    "                processed_line['year'] = js['year']\n",
    "            elif col == 'keywords':\n",
    "                processed_line['keywords'] = ';'.join(js['keywords'])\n",
    "            elif col == 'references':\n",
    "                processed_line['references'] = ';'.join([str(r) for r in js['references']])\n",
    "            elif col == 'n_citation':\n",
    "                processed_line['n_cites'] = js['n_citation']\n",
    "            elif col == 'doc_type':\n",
    "                processed_line['doc_type'] = js['doc_type']\n",
    "            elif col == 'fos':\n",
    "                fos = js['fos']\n",
    "                processed_line['fos_name'] = ';'.join([str(f.get('name', None))\n",
    "                                                       for f in fos if f.get('name', None)])\n",
    "                processed_line['fos_weight'] = ';'.join([str(f.get('w', None))\n",
    "                                                         for f in fos if f.get('w', None)])\n",
    "        return processed_line\n",
    "\n",
    "    with open(file, encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    def process(ls):\n",
    "        df = pd.DataFrame()\n",
    "        for line in ls:\n",
    "            line = line.lstrip('[')\n",
    "            line = line.lstrip(',')\n",
    "            line = line.rstrip(']')\n",
    "            line = line.rstrip('\\n')\n",
    "            js = json.loads(line)\n",
    "            df = df.append(parse(js), ignore_index=True)\n",
    "        return df\n",
    "\n",
    "    parsed_df = multiprocess(process, split_data(lines[1: -1], size=2000))\n",
    "\n",
    "    # Change data type\n",
    "    parsed_df['id'] = parsed_df['id'].astype('int64')\n",
    "    parsed_df['year'] = parsed_df['year'].astype('int')\n",
    "    parsed_df['n_cites'] = parsed_df['n_cites'].astype('int')\n",
    "\n",
    "    return parsed_df\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}