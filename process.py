import pandas as pd
import numpy as np

import os
from utils import read_csv, multiprocess, split_data
from config import DATA_PATH
from joblib import Parallel, delayed
from tqdm import tqdm


def overview(paper_df, by='country'):
    """ Data for the annual publication overview on the certain scale.

    Args:
        - paper_df: pd.Dataframe
            paper data, requires column of id, year & affiliation
        - by: string, 'country' or 'city'
            dimension where we calculate the annual publication num

    Returns:
        processed Dataframe, grouped by year & country/city

    """

    def process(df):
        df_ = df[['id', 'year', by]].copy()
        df_ = df_.dropna(subset=[by])
        grouped_paper = df_.groupby(["year", by])

        df1 = pd.DataFrame()
        count = grouped_paper["id"].apply(len)
        df1["year"] = [idx[0] for idx in count.axes[0]]
        df1[by] = [idx[1] for idx in count.axes[0]]
        df1["publication count"] = count.values.astype('int')

        df1['year'] = df1['year'].astype('int')
        return df1

    overview_df_ = multiprocess(process, split_data(paper_df, by='year'), n_jobs=12)
    grouped_overview_df = overview_df_.groupby(["year", by])

    overview_df = pd.DataFrame()
    count_sum = grouped_overview_df["publication count"].apply(sum)
    overview_df["year"] = [idx[0] for idx in count_sum.axes[0]]
    overview_df[by] = [idx[1] for idx in count_sum.axes[0]]
    overview_df["publication count"] = count_sum.values.astype('int')

    overview_df['year'] = overview_df['year'].astype('int')
    return overview_df


def annual_growth_rate(pub_df, start=2000, by='country'):
    """ Calculate the annual growth rate of publication by annual publication

    To select frequently-cited or A papers,
    Args:
        - pub_df: pd.Dataframe
            publication data, generated by overview()
        - start: int, default 2000
            start date for calculating growth rate
        - by: string, 'country' or 'city'
            dimension where we calculate the annual publication growth rate

    Returns:
        processed Dataframe, grouped by year & country in format percentage
        if the region had no publication last year, growth rate is np.nan

    """
    gr_df = pd.DataFrame()
    for year in range(start, max(pub_df['year']) + 1):
        for region in pd.unique(pub_df[by]):
            region_df = pub_df[pub_df[by] == region].copy().reset_index(drop=True)
            l = region_df[region_df['year'] == year - 1]['publication count'].values
            t = region_df[region_df['year'] == year]['publication count'].values
            if l.size > 0:
                if t.size > 0:
                    gr = 100. * (t[0] - l[0]) / l[0]
                else:
                    gr = -100.
            else:
                if t.size > 0:
                    gr = np.nan
                else:
                    gr = 0.
            gr_df = gr_df.append({'year': year, by: region, 'growth rate': gr}, ignore_index=True)
    gr_df['year'] = gr_df['year'].astype('int')
    return gr_df


def keywords_freq(paper_df, year):
    """ Calculate keywords frequency according to paper_df

    For now, only dblp dataset contains keywords column
    Args:
        - paper_df: pd.Dataframe
            dblp data
        - year: int
            year to calculate keyword frequency
    Returns:
        pd.Dataframe
        year | keyword1 | keyword2 | ...
        ________________________________
             |          |          | ...
    """
    keywords = paper_df[paper_df['year'] == year]['keywords'].values
    count = dict()
    for l in keywords:
        for k in l.split(';'):
            count[k] = count.get(k, 0) + 1
    return count


# def fos_names(paper_df):
#     def process(df):
#         fnames = set()
#         for name in df['fos_name'].values:
#             fnames = fnames.union(set(name.split(';')))
#         return fnames
#
#     fos_names = set()
#     results = Parallel(n_jobs=12)(delayed(process)(d) for d in tqdm(paper_df))
#     fos_names = fos_names.union(*results)
#     return list(fos_names)

def fos_names(df):
    fnames = set()
    for name in df['fos_name'].values:
        fnames = fnames.union(set(name.split(';')))
    return list(fnames)


# def fos_freq(paper_df):
#     def process


####################################### COAUTHOR ###################################################
# 计算合作强度
def cooperate_strength(coauthor_df, author_df, by='country'):
    """ Calculate cooperate strength (times)

    Args:
        - coauthor_df: pd.Dataframe
            coauthor data
        - coauthor_df: pd.Dataframe
            author data, requires column affiliation
        - by: string, either 'country', 'city' or 'affiliation'
            dimension where authors aggregates
    Returns:
        pd.Dataframe
        1st | 2nd | str
        _______________
            |     |
    """
    assert by in ['country', 'city', 'affiliation']

    def get(id):
        res = author_df[author_df['author_id'] == id][by].values
        if len(res) > 0:
            return res[0]

    def process(df):
        coop_str = pd.DataFrame()
        coop_str['1st'] = df['1st'].apply(get)
        coop_str['2nd'] = df['2nd'].apply(get)
        coop_str['str'] = df['num']
        coop_str = coop_str.dropna(subset=['1st', '2nd'])
        return coop_str

    coop_df = multiprocess(process, split_data(coauthor_df, size=1000))

    df1 = pd.DataFrame()
    groups = coop_df.groupby(by=['1st', '2nd'])
    strength = groups['str'].apply(sum)
    df1['1st'] = [idx[0] for idx in strength.axes[0]]
    df1['2nd'] = [idx[1] for idx in strength.axes[0]]
    df1['strength'] = strength.values.astype('int')
    return df1


if __name__ == '__main__':

    ####################################### PREPROCESS ##################################################
#     # In asn author dataset, an author can have several affiliations, choose the first one.
#     # author_df['affiliation'] = author_df['affiliations'].apply(lambda s: s.split(';')[0] if s else np.nan)
#
#     # For now, the author attribute for paper is the first of the authors.
#     # dblp_paper_df['affiliation'] = dblp_paper_df['authors_org'].apply(lambda s: s.split(';')[0] if s else np.nan)
#     # asn_paper_df['author_name'] = asn_paper_df['authors_name'].apply(lambda s: s.split(';')[0] if s else np.nan)
    dblp_paper_df = read_csv(DATA_PATH, 'dblp_paper.csv')
#     paper_df = dblp_paper_df.dropna(subset=['authors_org']).copy().reset_index(drop=True)
#     paper_df['affiliation'] = paper_df['authors_org'].apply(lambda s: s.split(';')[0] if s else None)
#     global_overview_df = overview(paper_df)
#     global_overview_df.to_csv(os.path.join(DATA_PATH, 'global_overview.csv'), index=False)
#
#     growth_rate_df = annual_growth_rate(global_overview_df)
#     growth_rate_df.to_csv(os.path.join(DATA_PATH, 'global_overview.csv'), index=False)
#
#     # For the sample dblp dataset (LIMIT 10000), we have no keywords, hence this part
#     # keywords_freq_df = pd.DataFrame()
#     # paper_df = dblp_paper_df.dropna(subset=['year', 'keywords'])
#     # for year in pd.unique(paper_df['year']):
#     #     kf = {'year': year}
#     #     kf.update(keywords_freq(paper_df, year))
#     #     keywords_freq_df.append(kf)
#     # keywords_freq_df.fillna(0)
#
#     author_df = read_csv(DATA_PATH, 'author.csv', index='author_id')
    fos_df = pd.DataFrame()
    fos_df['fos'] = fos_names(dblp_paper_df)
    fos_df.to_csv(os.path.join(DATA_PATH, 'fos_freq.csv'), index=False)
