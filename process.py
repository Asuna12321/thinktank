import pandas as pd
import numpy as np

import os
from utils import read_csv, multiprocess, split_data
from config import DATA_PATH

from collections import Counter
from joblib import Parallel, delayed
from tqdm import tqdm


def overview(paper_df, by='country'):
    """ Data for the annual publication overview on the certain scale.

    Args:
        - paper_df: pd.Dataframe
            paper data, requires column of id, year & affiliation
        - by: string, 'country' or 'city'
            dimension where we calculate the annual publication num

    Returns:
        processed Dataframe, grouped by year & country/city

    """

    def process(df):
        df_ = df[['id', 'year', by]].copy()
        df_ = df_.dropna(subset=[by])
        grouped_paper = df_.groupby(["year", by])

        df1 = pd.DataFrame()
        count = grouped_paper["id"].apply(len)
        df1["year"] = [idx[0] for idx in count.axes[0]]
        df1[by] = [idx[1] for idx in count.axes[0]]
        df1["publication count"] = count.values.astype('int')

        df1['year'] = df1['year'].astype('int')
        return df1

    overview_df_ = multiprocess(process, split_data(paper_df, by='year'), n_jobs=12)
    grouped_overview_df = overview_df_.groupby(["year", by])

    overview_df = pd.DataFrame()
    count_sum = grouped_overview_df["publication count"].apply(sum)
    overview_df["year"] = [idx[0] for idx in count_sum.axes[0]]
    overview_df[by] = [idx[1] for idx in count_sum.axes[0]]
    overview_df["publication count"] = count_sum.values.astype('int')

    overview_df['year'] = overview_df['year'].astype('int')
    return overview_df


def annual_growth_rate(pub_df, start=2000, by='country'):
    """ Calculate the annual growth rate of publication by annual publication

    To select frequently-cited or A papers,
    Args:
        - pub_df: pd.Dataframe
            publication data, generated by overview()
        - start: int, default 2000
            start date for calculating growth rate
        - by: string, 'country' or 'city'
            dimension where we calculate the annual publication growth rate

    Returns:
        processed Dataframe, grouped by year & country in format percentage
        if the region had no publication last year, growth rate is np.nan

    """
    gr_df = pd.DataFrame()
    for year in range(start, max(pub_df['year']) + 1):
        for region in pd.unique(pub_df[by]):
            region_df = pub_df[pub_df[by] == region].copy().reset_index(drop=True)
            l = region_df[region_df['year'] == year - 1]['publication count'].values
            t = region_df[region_df['year'] == year]['publication count'].values
            if l.size > 0:
                if t.size > 0:
                    gr = 100. * (t[0] - l[0]) / l[0]
                else:
                    gr = -100.
            else:
                if t.size > 0:
                    gr = np.nan
                else:
                    gr = 0.
            gr_df = gr_df.append({'year': year, by: region, 'growth rate': gr}, ignore_index=True)
    gr_df['year'] = gr_df['year'].astype('int')
    return gr_df


def keywords_freq(paper_df, year):
    """ Calculate keywords frequency according to paper_df

    For now, only dblp dataset contains keywords column
    Args:
        - paper_df: pd.Dataframe
            dblp data
        - year: int
            year to calculate keyword frequency
    Returns:
        pd.Dataframe
        year | keyword1 | keyword2 | ...
        ________________________________
             |          |          | ...
    """
    keywords = paper_df[paper_df['year'] == year]['keywords'].values
    count = dict()
    for l in keywords:
        for k in l.split(';'):
            count[k] = count.get(k, 0) + 1
    return count


def fos_freq(paper_df):
    def process(df):
        fos_count = Counter()
        for name in df['fos_name'].values:
            fos_count += Counter(str(name).split(';'))
        return fos_count

    fos_count = Counter()
    data = split_data(paper_df.dropna(subset=['fos_name']), size=1000)
    results = Parallel(n_jobs=12)(delayed(process)(d) for d in tqdm(data))
    for result in results:
        fos_count += result
    fos_freq_df = pd.DataFrame()
    fos_freq_df['fos_name'] = fos_count.keys()
    fos_freq_df['freq'] = fos_count.values()
    return fos_freq_df


def region_str(author_df, by='country'):
    df = author_df.dropna(subset=[by]).copy().reset_index(drop=True)
    df = df[['n_cites', by]]

    def h_index(df):
        processed_df = df.sort_values(by=['n_cites'], ascending=False).copy().reset_index(drop=True)
        index = processed_df.shape[0]
        for i in range(0, processed_df.shape[0]):
            if int(processed_df['n_cites'][i]) <= i:
                index = i
        return pd.DataFrame({by: [df[by].values[0]], 'h_index': [index]})

    # h_index(split_data(df, by=by)[0])
    reg_str = multiprocess(h_index, split_data(df, by=by), n_jobs=12)
    return reg_str


####################################### COAUTHOR ###################################################
# 计算合作强度
def cooperate_strength(coauthor_df, author_df, by='country'):
    """ Calculate cooperate strength (times)

    Args:
        - coauthor_df: pd.Dataframe
            coauthor data
        - coauthor_df: pd.Dataframe
            author data, requires column affiliation
        - by: string, either 'country', 'city' or 'affiliation'
            dimension where authors aggregates
    Returns:
        pd.Dataframe
        1st | 2nd | str
        _______________
            |     |
    """
    assert by in ['country', 'city', 'affiliation']

    def get(id):
        res = author_df[author_df['author_id'] == id][by].values
        if len(res) > 0:
            return res[0]

    def process(df):
        coop_str = pd.DataFrame()
        coop_str['1st'] = df['1st'].apply(get)
        coop_str['2nd'] = df['2nd'].apply(get)
        coop_str['str'] = df['num']
        coop_str = coop_str.dropna(subset=['1st', '2nd'])
        return coop_str

    coop_df = multiprocess(process, split_data(coauthor_df, size=1000))

    df1 = pd.DataFrame()
    groups = coop_df.groupby(by=['1st', '2nd'])
    strength = groups['str'].apply(sum)
    df1['1st'] = [idx[0] for idx in strength.axes[0]]
    df1['2nd'] = [idx[1] for idx in strength.axes[0]]
    df1['strength'] = strength.values.astype('int')
    return df1


if __name__ == '__main__':

#     ####################################### PREPROCESS ##################################################
#     # In asn author dataset, an author can have several affiliations, choose the first one.
#     # author_df['affiliation'] = author_df['affiliations'].apply(lambda s: s.split(';')[0] if s else np.nan)
#
#     # For now, the author attribute for paper is the first of the authors.
#     # dblp_paper_df['affiliation'] = dblp_paper_df['authors_org'].apply(lambda s: s.split(';')[0] if s else np.nan)
#     # asn_paper_df['author_name'] = asn_paper_df['authors_name'].apply(lambda s: s.split(';')[0] if s else np.nan)
#     dblp_paper_df = read_csv(DATA_PATH, 'dblp_paper.csv')
# #     paper_df = dblp_paper_df.dropna(subset=['authors_org']).copy().reset_index(drop=True)
# #     paper_df['affiliation'] = paper_df['authors_org'].apply(lambda s: s.split(';')[0] if s else None)
# #     global_overview_df = overview(paper_df)
# #     global_overview_df.to_csv(os.path.join(DATA_PATH, 'global_overview.csv'), index=False)
# #
# #     growth_rate_df = annual_growth_rate(global_overview_df)
# #     growth_rate_df.to_csv(os.path.join(DATA_PATH, 'global_overview.csv'), index=False)
# #
# #     # For the sample dblp dataset (LIMIT 10000), we have no keywords, hence this part
# #     # keywords_freq_df = pd.DataFrame()
# #     # paper_df = dblp_paper_df.dropna(subset=['year', 'keywords'])
# #     # for year in pd.unique(paper_df['year']):
# #     #     kf = {'year': year}
# #     #     kf.update(keywords_freq(paper_df, year))
# #     #     keywords_freq_df.append(kf)
# #     # keywords_freq_df.fillna(0)
# #
# #     author_df = read_csv(DATA_PATH, 'author.csv', index='author_id')
#     for year in range(2010, 2020):
#         print(f'START YEAR {year}')
#         fos_freq_df = fos_freq(dblp_paper_df[dblp_paper_df['year'] == year])
#         fos_freq_df.to_csv(os.path.join(DATA_PATH, f'fos_freq_{year}.csv'), index=False)
    paper_df = read_csv(DATA_PATH, 'paper.csv')
    region_str(paper_df)
